{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNu/Y9qyVQ1SunBXQHDJetF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nuentsa/cloud-data-tools/blob/main/runtastic_data/extract_and_load_adidas_running_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Option 1: Retrieve the Runtastic Zip File from Google Drive\n",
        "# This is the fastest option to get the file as it is readily available in Google drive to process multiple times\n",
        "\n",
        "from pickle import TRUE\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# We will save and unzip  the  runtastic file here.\n",
        "work_dir = \"/home/runtastic/\"\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "runtastic_file_path = os.path.join(work_dir, \"runtastic_data_export.zip\")\n",
        "\n",
        "\n",
        "# This File ID is the identifier of your zip file stored in Google Drive\n",
        "# Oonce you store the zip file in Google drive, retrieve its identifier to store it as secrets in Google Colab\n",
        "# It should look like this laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "runtastic_file_id = userdata.get('runtastic_file_id')\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "\n",
        "uploaded_file = drive.CreateFile({'id': runtastic_file_id})\n",
        "uploaded_file.GetContentFile(runtastic_file_path)\n"
      ],
      "metadata": {
        "id": "dlYlkFRkK32Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Option 2: Retrieve the Runtastic Zip file from local computer\n",
        "# This is the slowest option as you will need to upload it anytime you want to process the file\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# We will save and unzip  the  runtastic file here.\n",
        "work_dir = \"/home/runtastic/\"\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "runtastic_file_path = os.path.join(work_dir, \"runtastic_data_export.zip\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "shutil.move(uploaded_file_name, runtastic_file_path)"
      ],
      "metadata": {
        "id": "YTgGORe2Kea-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unzip the data\n",
        "# Extract all files from the downloaded archive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "#   Recursively displays the folders beneath a specific folder in hierarchical way.\n",
        "def display_folders(folder_path, indent=0):\n",
        "  # Get all subfolders and files in the current folder\n",
        "  subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
        "  files = [f for f in os.listdir(folder_path) if not os.path.isdir(os.path.join(folder_path, f))]\n",
        "  # Print the current folder name with indentation\n",
        "  print(\" \" * indent + folder_path)\n",
        "  # Recursively display subfolders\n",
        "  for subfolder in subfolders:\n",
        "    display_folders(os.path.join(folder_path, subfolder), indent + 2)\n",
        "\n",
        "# Open the zip file and extract it\n",
        "zipobject = zipfile.ZipFile(runtastic_file_path)\n",
        "# Unzip all files in a specific directory\n",
        "zipobject.extractall(work_dir)\n",
        "\n",
        "# Display the contents of the extracted zip\n",
        "display_folders(work_dir)\n",
        "\n",
        "# List all sport sessions\n",
        "session_files = []\n",
        "for filename in os.listdir(os.path.join(work_dir, \"Sport-sessions\")):\n",
        "  if filename.endswith(\".json\"):\n",
        "    session_files.append(os.path.join(work_dir, \"Sport-sessions\", filename))\n"
      ],
      "metadata": {
        "id": "8YuLxMtzODGX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Global attributes to be used throughout the notebook\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import os\n",
        "# Global attributes\n",
        "# We will retrieve these specific top-level fields from each sport session file\n",
        "top_level_fields = [\n",
        "    \"version\", \"duration\", \"pause\", \"calories\", \"dehydration_volume\",\n",
        "    \"start_time_timezone_offset\", \"end_time_timezone_offset\", \"start_time\",\n",
        "    \"end_time\", \"created_at\", \"updated_at\", \"id\", \"sport_type_id\"\n",
        "]\n",
        "# Each sport session has a various features depending on the sport type. We will load only the following ones from running/walking/cycling sessions\n",
        "features_to_load = [\"weather\", \"map\", \"track_metrics\", \"initial_values\"]\n",
        "supported_feature_attributes = [\"temperature\", \"wind_speed\", \"wind_direction\",\n",
        "                                \"humidity\", \"start_latitude\", \"start_longitude\",\n",
        "                                \"distance\", \"average_speed\", \"average_pace\",\n",
        "                                \"max_speed\", \"elevation_gain\", \"elevation_loss\", \"distance\", \"duration\"]\n",
        "\n",
        "sport_types = [\n",
        "  [1, 'Running'],\n",
        "  [62, 'Speed Skiing'],\n",
        "  [2, 'Nordic Walking'],\n",
        "  [63, 'PushUps'],\n",
        "  [3, 'Cycling'],\n",
        "  [64, 'SitUps'],\n",
        "  [4, 'Mountain Biking'],\n",
        "  [65, 'PullUps'],\n",
        "  [5, 'Other'],\n",
        "  [66, 'Squats'],\n",
        "  [6, 'Inline Skating'],\n",
        "  [7, 'Hiking'],\n",
        "  [68, 'Baseball'],\n",
        "  [8, 'Cross-country skiing'],\n",
        "  [69, 'Crossfit'],\n",
        "  [9, 'Skiing'],\n",
        "  [70, 'Dancing'],\n",
        "  [10, 'Snowboarding'],\n",
        "  [71, 'Ice Hockey'],\n",
        "  [11, 'Motorbike'],\n",
        "  [72, 'Skateboarding'],\n",
        "  [13, 'Snowshoeing'],\n",
        "  [73, 'Zumba'],\n",
        "  [14, 'Treadmill'],\n",
        "  [74, 'Gymnastics'],\n",
        "  [15, 'Ergometer'],\n",
        "  [75, 'Rugby'],\n",
        "  [16, 'Elliptical'],\n",
        "  [76, 'Standup Paddling'],\n",
        "  [17, 'Rowing'],\n",
        "  [77, 'Sixpack'],\n",
        "  [18, 'Swimming'],\n",
        "  [78, 'Butt Training'],\n",
        "  [19, 'Walking'],\n",
        "  [80, 'Leg Training'],\n",
        "  [20, 'Riding'],\n",
        "  [81, 'Results Workout'],\n",
        "  [21, 'Golfing'],\n",
        "  [82, 'Trail Running'],\n",
        "  [22, 'Race Cycling'],\n",
        "  [84, 'Plogging'],\n",
        "  [23, 'Tennis'],\n",
        "  [85, 'Wheelchair'],\n",
        "  [24, 'Badminton'],\n",
        "  [86, 'E Biking'],\n",
        "  [25, 'Squash'],\n",
        "  [87, 'Scootering'],\n",
        "  [26, 'Yoga'],\n",
        "  [88, 'Rowing Machine'],\n",
        "  [27, 'Aerobics'],\n",
        "  [89, 'Stair Climbing'],\n",
        "  [28, 'Martial Arts'],\n",
        "  [90, 'Jumping Rope'],\n",
        "  [29, 'Sailing'],\n",
        "  [91, 'Trampoline'],\n",
        "  [30, 'Windsurfing'],\n",
        "  [92, 'Bodyweight Training'],\n",
        "  [31, 'Pilates'],\n",
        "  [93, 'Tabata'],\n",
        "  [32, 'Rock Climbing'],\n",
        "  [94, 'Callisthenics'],\n",
        "  [33, 'Frisbee'],\n",
        "  [95, 'Suspension Training'],\n",
        "  [34, 'Strength Training'],\n",
        "  [96, 'Powerlifting'],\n",
        "  [35, 'Volleyball'],\n",
        "  [97, 'Olympic Weightlifting'],\n",
        "  [36, 'Handbike'],\n",
        "  [98, 'Stretching'],\n",
        "  [37, 'Cross Skating'],\n",
        "  [99, 'Mediation'],\n",
        "  [38, 'Soccer'],\n",
        "  [100, 'Bouldering'],\n",
        "  [42, 'Surfing'],\n",
        "  [101, 'Via Ferrata'],\n",
        "  [43, 'Kitesurfing'],\n",
        "  [102, 'Pade'],\n",
        "  [44, 'Kayaking'],\n",
        "  [103, 'Pole Dancing'],\n",
        "  [45, 'Basketball'],\n",
        "  [104, 'Boxing'],\n",
        "  [46, 'Spinning'],\n",
        "  [105, 'Cricket']\n",
        "]\n",
        "# Create a dataframe with the sport types\n",
        "sport_types_df =  pd.DataFrame(sport_types, columns=['sport_type_id', 'activity'])\n",
        "sport_types_df.info(verbose=True)"
      ],
      "metadata": {
        "id": "Qbt5Yp0k48wn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load all Sport Sessions into Dataframes\n",
        "import pandas as pd\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "def process_features(features, id):\n",
        "  \"\"\"\n",
        "  Function to extract and flatten features data, returning the features as a dictionary\n",
        "  We will use this function to extract and flatten data in the features field\n",
        "  Each feature has specific metrics and data points : weather, location,\n",
        "  A list of fastest segments is provided separately as a list of dictionaries.\n",
        "\n",
        "  Args:\n",
        "    features: list of features\n",
        "    id: id of the current session file\n",
        "  Return:\n",
        "    feature_values: dictionary of values of interest extracted from each relevant feature\n",
        "    fastest_segments: list of segments enriched by the sport session id\n",
        "  \"\"\"\n",
        "  feature_values = {}\n",
        "  segments = []\n",
        "  for feature in features:\n",
        "    if feature[\"type\"] == \"fastest_segments\":\n",
        "      fastest_segments = feature[\"attributes\"][\"segments\"]\n",
        "      segments = extract_fastest_segments(id, fastest_segments)\n",
        "      continue\n",
        "    if feature[\"type\"] not in features_to_load:\n",
        "      continue\n",
        "    for key, value in feature[\"attributes\"].items():\n",
        "      if isinstance(value, dict):\n",
        "        # We will not support the embedding json object here\n",
        "        continue\n",
        "      if key not in supported_feature_attributes:\n",
        "        # We are not interested in this data dimension for now\n",
        "        continue\n",
        "      feature_values[key]=value\n",
        "  return feature_values, segments\n",
        "\n",
        "def extract_fastest_segments(id, array_of_segments):\n",
        "  \"\"\"\n",
        "  Retrieve the list of fastest segments beneath the features record\n",
        "  It has the form\n",
        "\n",
        "\n",
        "  Args:\n",
        "    array_of_segments: the json array having the list of segments\n",
        "    id: id of the sport session\n",
        "  \"\"\"\n",
        "  segments = []\n",
        "  segment_id = 0\n",
        "  for segment in array_of_segments:\n",
        "    segment[\"sessions_id\"] = id\n",
        "    # Generate a unique id for this segment within the context of this session\n",
        "    segment[\"id\"] = f\"{id}-{segment_id}\"\n",
        "    segments.append(segment)\n",
        "    segment_id += 1\n",
        "  return segments\n",
        "\n",
        "# Initialize empty DataFrame to store the sport sessions and the fastest segments\n",
        "sessions_df = pd.DataFrame()\n",
        "fastest_segments_df = pd.DataFrame()\n",
        "# Read each JSON file, process features, and append data to the dataframe\n",
        "try:\n",
        "  for filename in session_files:\n",
        "    with open(filename, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "      # Extract core data\n",
        "      loaded_data = {}\n",
        "      for col in top_level_fields:\n",
        "        loaded_data[col] = data[col]\n",
        "      # Process features and get a dictionary of feature values\n",
        "      feature_values, segments = process_features(data[\"features\"], data[\"id\"])\n",
        "      loaded_data.update(feature_values)\n",
        "      current_row_df = pd.DataFrame.from_dict(loaded_data, orient='index')\n",
        "      sessions_df = pd.concat([sessions_df, current_row_df.T], ignore_index=True)\n",
        "\n",
        "      current_segments_df = pd.DataFrame.from_dict(segments)\n",
        "      fastest_segments_df = pd.concat([fastest_segments_df, current_segments_df], ignore_index=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error processing file '{filename}': {e}\")\n",
        "\n",
        "\n",
        "sessions_df.head()\n"
      ],
      "metadata": {
        "id": "9ACrzA3_ZuG5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply Some Data Transformations\n",
        "\n",
        "# Explicitly set the types of some columns\n",
        "sessions_df['average_speed'] = sessions_df['average_speed'].astype(float)\n",
        "sessions_df['max_speed'] = sessions_df['max_speed'].astype(float)\n",
        "sessions_df['average_pace'] = sessions_df['average_pace'].astype(float)\n",
        "sessions_df['start_longitude'] = sessions_df['start_longitude'].astype(float)\n",
        "sessions_df['start_latitude'] = sessions_df['start_latitude'].astype(float)\n",
        "\n",
        "# Handle non provided values\n",
        "# Some features are not present in all sessions like temperature or humidity,\n",
        "# which lead to NaN fields\n",
        "nan_columns = sessions_df.columns[sessions_df.isnull().any()]\n",
        "if len(nan_columns) > 0:\n",
        "  print('data dimensions with NaN values: ', end=' ')\n",
        "  print(' '.join(nan_columns))\n",
        "# Replace them with a predefined sentinel value\n",
        "sentinel_value = -99999\n",
        "sessions_df.fillna(sentinel_value, inplace=True)\n",
        "\n",
        "# Convert time columns to UTC\n",
        "time_cols = ['start_time', 'end_time', 'created_at', 'updated_at']\n",
        "sessions_df[time_cols] = sessions_df[time_cols].apply(\n",
        "    lambda col: pd.to_datetime(col, unit='ms', utc=True), axis=1\n",
        ")\n",
        "fastest_segments_df['started_at'] = pd.to_datetime(fastest_segments_df['started_at'], unit='ms', utc=True)\n",
        "\n",
        "# Convert all duration to seconds instead of milliseconds\n",
        "duration_cols = ['duration', 'pause', 'start_time_timezone_offset', 'end_time_timezone_offset' ]\n",
        "for col in duration_cols:\n",
        "  sessions_df[col] = sessions_df[col] / 1000\n",
        "fastest_segments_df['duration'] = fastest_segments_df['duration'] / 1000\n",
        "\n",
        "# Convert the speed from meters/seconds to kilometers/hour\n",
        "sessions_df['average_speed'] = sessions_df['average_speed'].multiply(3.6)\n",
        "sessions_df['max_speed'] = sessions_df['max_speed'].multiply(3.6)\n",
        "\n",
        "# Add sport_type activity name using the sport_type_ id in each session.\n",
        "sessions_df['sport_type_id'] = sessions_df['sport_type_id'].astype(int)\n",
        "sessions_df = sessions_df.merge(sport_types_df, on='sport_type_id')\n",
        "\n",
        "sessions_df.info(verbose=True)"
      ],
      "metadata": {
        "id": "gU-CJ7aLZqsW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastest_segments_df.info(verbose=True)"
      ],
      "metadata": {
        "id": "q9Ng3eDnTqR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display the number of sport sessions per activity type\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sport_type_counts = sessions_df.groupby('activity')['activity'].count()\n",
        "plt.figure(figsize=(6, 6))  # Adjust figure size as needed\n",
        "plt.pie(sport_type_counts, labels=activity_names, autopct='%1.1f%%')\n",
        "plt.title('Distribution of Sport Sessions by Activity Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0YQ9ju6QwTN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute The Total Distance Per Month\n",
        "import calendar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract month and year from start_time and create new columns 'month' and 'year'\n",
        "sessions_df['month'] = sessions_df['start_time'].dt.month\n",
        "sessions_df['year'] = sessions_df['start_time'].dt.year\n",
        "\n",
        "# Map month number to month name\n",
        "sessions_df['month'] = sessions_df['month'].apply(lambda x: calendar.month_name[x])\n",
        "\n",
        "# Group activity per month and year and calculate sum of distance\n",
        "distance_per_month_year = sessions_df.groupby(['year', 'month'])['distance'].sum()/1000\n",
        "\n",
        "# Plot sum of distance per month\n",
        "plt.figure(figsize=(12, 8))  # Adjust figure size\n",
        "distance_per_month_year.plot(kind='bar', xlabel='Month, Year', ylabel='Total Distance (km)', title='Total Distance per Month and Year', figsize=(10, 6))\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()  # Adjust layout\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1QumkjWpLFjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute the cumulative distance and duration over time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'start_time' column is in datetime format, if not convert it using pd.to_datetime()\n",
        "sessions_df['start_time'] = pd.to_datetime(sessions_df['start_time'])\n",
        "\n",
        "# Sort the dataframe by start_time\n",
        "sessions_df.sort_values(by='start_time', inplace=True)\n",
        "\n",
        "# Calculate cumulative sum of distance and duration\n",
        "sessions_df['cumulative_distance'] = sessions_df['distance'].cumsum() / 1000\n",
        "sessions_df['cumulative_duration'] = sessions_df['duration'].cumsum() / 3600\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot cumulative distance over time\n",
        "ax1.plot(sessions_df['start_time'], sessions_df['cumulative_distance'], color='blue')\n",
        "ax1.set_xlabel('Time')\n",
        "ax1.set_ylabel('Cumulative Distance (km)')\n",
        "ax1.set_title('Cumulative Distance Over Time')\n",
        "\n",
        "# Plot cumulative duration over time\n",
        "ax2.plot(sessions_df['start_time'], sessions_df['cumulative_duration'], color='orange')\n",
        "ax2.set_xlabel('Time')\n",
        "ax2.set_ylabel('Cumulative Duration (hours)')\n",
        "ax2.set_title('Cumulative Duration Over Time')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BsllS7xENYT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute the distribution of Sessions Durations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(17, 6))\n",
        "\n",
        "#  Duration distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(sessions_df[\"duration\"]/60, bins=20, edgecolor='black')\n",
        "plt.xlabel(\"Duration (minutes)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Session Durations\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Distance versus duration\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(sessions_df[\"duration\"]/60, sessions_df[\"distance\"]/1000)\n",
        "plt.xlabel(\"Duration (minutes)\")\n",
        "plt.ylabel(\"Distance (kilometers)\")\n",
        "plt.title(\"Distance vs. Duration\")\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-DPo6X-6VCbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IuLtUxdYK6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastest_segments_df.columns\n",
        "fastest_segments_df.head()"
      ],
      "metadata": {
        "id": "AkuHTG4hTUSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display the distribution of segment duration vs distance\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'started_at' column is in datetime format, if not convert it using pd.to_datetime()\n",
        "fastest_segments_df['started_at'] = pd.to_datetime(fastest_segments_df['started_at'])\n",
        "\n",
        "plt.figure(figsize=(17, 6))\n",
        "# Create a scatter plot of segment duration vs. distance\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(fastest_segments_df['distance'], fastest_segments_df['duration']/60, alpha=0.5)\n",
        "plt.xlabel('Segment Distance')\n",
        "plt.ylabel('Segment Duration (min)')\n",
        "plt.title('Segment Duration vs. Distance')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(sessions_df['start_time'], sessions_df['average_speed'], color='orange')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Average Spped (km/h)')\n",
        "plt.title('Average Speed Over Time')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HGlItGRfUujI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the data into  postgres for further analysis\n",
        "import sqlalchemy as sa\n",
        "\n",
        "# Extract the connection string from the notebook secrets\n",
        "conn_string = userdata.get('postgres_conn_string')\n",
        "engine = sa.create_engine(conn_string)\n",
        "\n",
        "sessions_table = \"sessions\"\n",
        "fastest_segments_table = \"fastest_segments\"\n",
        "\n",
        "# Load the sessions data\n",
        "sessions_df.to_sql(sessions_table, engine, if_exists='replace')\n",
        "\n",
        "# Load the fastest segments\n",
        "fastest_segments_df.to_sql(fastest_segments_table, engine, if_exists='replace')\n",
        "\n",
        "# Destroy the engine\n",
        "engine.dispose()\n",
        "\n",
        "print(\"All data have been loaded successfully in postgres\")\n",
        "\n",
        "# TODO Add primary and foreign keys"
      ],
      "metadata": {
        "id": "9k_qkDfOMJgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}